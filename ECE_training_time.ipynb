{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECE_training_time.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasewark/Neural-Network-Experiments/blob/master/ECE_training_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADTNaKzFmvgq",
        "colab_type": "code",
        "outputId": "93e5bbcf-98b2-4e5a-d462-9ca2cf73906c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 134kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 41.7MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 58.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hl2BG7OmkYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#resnet from https://keras.io/examples/cifar10_resnet/\n",
        "from __future__ import print_function\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A5LYw9fpKba",
        "colab_type": "code",
        "outputId": "0889aef8-80b7-4a4d-c248-e1817690b5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 64  # orig paper trained all networks with batch_size=128\n",
        "epochs = 200\n",
        "data_augmentation = False\n",
        "num_classes = 10\n",
        "\n",
        "# Subtracting pixel mean improves accuracy\n",
        "subtract_pixel_mean = True\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 2\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "if version == 1:\n",
        "    depth = n * 6 + 2\n",
        "elif version == 2:\n",
        "    depth = n * 9 + 2\n",
        "\n",
        "# Model name, depth and version\n",
        "model_type = 'ResNet%dv%d' % (depth, version)\n",
        "\n",
        "# Load the CIFAR10 data.\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# If subtract pixel mean is enabled\n",
        "if subtract_pixel_mean:\n",
        "    x_train_mean = np.mean(x_train, axis=0)\n",
        "    x_train -= x_train_mean\n",
        "    x_test -= x_train_mean\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "print('y_train shape:', y_train.shape)\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "y_train shape: (50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yXlLq6GmJWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch > 180:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 160:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth, num_classes=10):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-WU63Nfu4s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ece_list=np.load('test_ece_list.npy').tolist()\n",
        "train_ece_list=np.load('train_ece_list.npy').tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Vq1P3f4uTe",
        "colab_type": "code",
        "outputId": "f6f1e926-6942-46ee-9929-950460fe0d6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(test_ece_list[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[       nan 0.02543121 0.03670059 0.09035782 0.06893428 0.06839215\n",
            " 0.11566135 0.06117817 0.06523531 0.15779337 0.11198218 0.20601563\n",
            " 0.10200115 0.12649395 0.12197527 0.14247008 0.10716674 0.13581289\n",
            " 0.15214253 0.12307996 0.14903724 0.10613755 0.16749297 0.123715\n",
            " 0.11930787 0.19923059 0.18565422 0.1483015  0.17797249 0.12931802\n",
            " 0.11311642 0.16415836 0.13932936 0.12469658 0.15487313 0.14958523\n",
            " 0.12770103 0.14007786 0.12517897 0.12683794 0.13440867 0.13115256\n",
            " 0.13300387 0.14049663 0.13964269 0.12306794 0.13814767 0.13015305\n",
            " 0.15601504 0.16747656 0.21442097 0.11797451 0.14429712 0.12497437\n",
            " 0.17515571 0.13255208 0.14516682 0.14162301 0.14323899 0.13969963\n",
            " 0.13148865 0.15471467 0.16407578 0.13129142 0.17593241 0.19987468\n",
            " 0.1455832  0.13873844 0.14291244 0.14492711 0.13753977 0.15588693\n",
            " 0.14426509 0.1676316  0.15300938 0.1251797  0.21524902 0.15696133\n",
            " 0.13222759 0.14828459 0.1607896  0.12401479 0.11634259 0.15821093\n",
            " 0.16685762 0.10916494 0.13934199 0.12490342 0.13993335 0.11922343\n",
            " 0.13043643 0.14585337 0.1379557  0.15462125 0.13805355 0.1331905\n",
            " 0.13890413 0.13084857 0.14747736 0.10592235]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lTa8_e7Mvew",
        "colab_type": "code",
        "outputId": "7864933e-da25-43f7-af3c-8b16fa660092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if version == 2:\n",
        "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
        "else:\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=lr_schedule(0)),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(model_type)\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "#save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "#model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
        "#if not os.path.isdir(save_dir):\n",
        "#    os.makedirs(save_dir)\n",
        "#filepath = os.path.join(save_dir, model_name)\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "#checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "#                             monitor='val_acc',\n",
        "#                             verbose=1,\n",
        "#                             save_best_only=True)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 32, 32, 16)   64          conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 32, 32, 16)   0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 32, 32, 16)   272         activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 32, 32, 16)   64          conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 32, 32, 16)   0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 32, 32, 16)   2320        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 32, 32, 16)   64          conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 32, 32, 16)   0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 32, 32, 64)   1088        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 32, 32, 64)   1088        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 32, 32, 64)   0           conv2d_66[0][0]                  \n",
            "                                                                 conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 32, 32, 64)   256         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 32, 32, 64)   0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 32, 32, 16)   1040        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 32, 32, 16)   64          conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 32, 32, 16)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 32, 32, 16)   2320        activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 32, 32, 16)   64          conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 32, 32, 16)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 32, 32, 64)   1088        activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 32, 32, 64)   0           add_18[0][0]                     \n",
            "                                                                 conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 32, 32, 64)   256         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 32, 32, 64)   0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 32, 32, 16)   1040        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 32, 32, 16)   64          conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 32, 32, 16)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 32, 32, 16)   2320        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 32, 32, 16)   64          conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 32, 32, 16)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 32, 32, 64)   1088        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 32, 32, 64)   0           add_19[0][0]                     \n",
            "                                                                 conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 32, 64)   256         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 32, 32, 64)   0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 16, 16, 64)   4160        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 16, 16, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 16, 16, 64)   36928       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 16, 16, 128)  8320        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 16, 16, 128)  8320        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 16, 16, 128)  0           conv2d_76[0][0]                  \n",
            "                                                                 conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 16, 16, 128)  512         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 16, 16, 128)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 16, 16, 64)   8256        activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 16, 16, 64)   256         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 16, 16, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 16, 16, 64)   36928       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 16, 16, 128)  8320        activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 16, 16, 128)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 16, 16, 128)  512         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 16, 16, 128)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 16, 16, 64)   8256        activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 16, 16, 64)   256         conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 16, 16, 64)   0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 16, 16, 64)   36928       activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 16, 16, 64)   256         conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 16, 16, 64)   0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 16, 16, 128)  8320        activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 16, 16, 128)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 16, 16, 128)  512         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 16, 16, 128)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 8, 8, 128)    16512       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 8, 8, 128)    512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 8, 8, 128)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 8, 8, 128)    147584      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 8, 8, 128)    512         conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 8, 8, 128)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 8, 8, 256)    33024       add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 8, 8, 256)    33024       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 8, 8, 256)    0           conv2d_86[0][0]                  \n",
            "                                                                 conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 8, 8, 256)    1024        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 8, 8, 256)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 8, 8, 128)    32896       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 8, 8, 128)    512         conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 8, 8, 128)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 8, 8, 128)    147584      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 8, 8, 128)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 8, 8, 256)    33024       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 8, 8, 256)    0           add_24[0][0]                     \n",
            "                                                                 conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 8, 8, 256)    1024        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 8, 8, 256)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 8, 8, 128)    32896       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 8, 8, 128)    512         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 8, 8, 128)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 8, 8, 128)    147584      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 8, 8, 128)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 8, 8, 256)    33024       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 8, 8, 256)    0           add_25[0][0]                     \n",
            "                                                                 conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 8, 8, 256)    1024        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 8, 8, 256)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 1, 1, 256)    0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 256)          0           average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "ResNet29v2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pV7WxIGM5kK",
        "colab_type": "code",
        "outputId": "4e219dac-1d2f-4b71-e4c1-05e034140ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_ece=[]\n",
        "train_ece=[]\n",
        "\n",
        "for i in range(100):\n",
        "  print('Epoch ',i)\n",
        "  pred_train=model.predict(x_train)\n",
        "  e_train=ECE(pred_train,y_train)\n",
        "  print(e_train)\n",
        "  train_ece.append(e_train)\n",
        "  pred=model.predict(x_test)\n",
        "  e=ECE(pred,y_test)\n",
        "  print(e)\n",
        "  test_ece.append(e)\n",
        "  model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=1,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch  0\n",
            "nan\n",
            "nan\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 105s 2ms/sample - loss: 1.8399 - accuracy: 0.5086 - val_loss: 1.8739 - val_accuracy: 0.4933\n",
            "Epoch  1\n",
            "0.137509228515625\n",
            "0.1492532958984375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.3255 - accuracy: 0.6605 - val_loss: 1.4073 - val_accuracy: 0.6325\n",
            "Epoch  2\n",
            "0.038089765625\n",
            "0.057583349609375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 1.1195 - accuracy: 0.7239 - val_loss: 1.2016 - val_accuracy: 0.6928\n",
            "Epoch  3\n",
            "0.013518360595703126\n",
            "0.0324651123046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.9879 - accuracy: 0.7651 - val_loss: 1.2025 - val_accuracy: 0.7055\n",
            "Epoch  4\n",
            "0.02373697509765625\n",
            "0.068704248046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.8914 - accuracy: 0.7965 - val_loss: 1.1632 - val_accuracy: 0.7057\n",
            "Epoch  5\n",
            "0.010435985107421875\n",
            "0.0455299072265625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.8189 - accuracy: 0.8203 - val_loss: 1.2801 - val_accuracy: 0.6954\n",
            "Epoch  6\n",
            "0.051459306640625\n",
            "0.10432652587890626\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.7596 - accuracy: 0.8403 - val_loss: 1.0537 - val_accuracy: 0.7426\n",
            "Epoch  7\n",
            "0.019050411376953125\n",
            "0.04744175720214844\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.7120 - accuracy: 0.8567 - val_loss: 1.0327 - val_accuracy: 0.7627\n",
            "Epoch  8\n",
            "0.02538581787109375\n",
            "0.05310462646484375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.6774 - accuracy: 0.8679 - val_loss: 1.1305 - val_accuracy: 0.7168\n",
            "Epoch  9\n",
            "0.020300303955078126\n",
            "0.057085894775390625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.6461 - accuracy: 0.8805 - val_loss: 1.2908 - val_accuracy: 0.7093\n",
            "Epoch  10\n",
            "0.0494251123046875\n",
            "0.12886865234375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.6172 - accuracy: 0.8906 - val_loss: 1.1319 - val_accuracy: 0.7490\n",
            "Epoch  11\n",
            "0.014150501708984376\n",
            "0.10270196533203126\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5955 - accuracy: 0.8996 - val_loss: 1.1525 - val_accuracy: 0.7519\n",
            "Epoch  12\n",
            "0.013061737060546875\n",
            "0.1108048583984375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.5836 - accuracy: 0.9041 - val_loss: 1.1894 - val_accuracy: 0.7429\n",
            "Epoch  13\n",
            "0.027519560546875\n",
            "0.11064942626953125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5598 - accuracy: 0.9121 - val_loss: 1.2793 - val_accuracy: 0.7184\n",
            "Epoch  14\n",
            "0.02739014404296875\n",
            "0.1302697998046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5484 - accuracy: 0.9175 - val_loss: 1.3195 - val_accuracy: 0.7232\n",
            "Epoch  15\n",
            "0.0395861474609375\n",
            "0.1416776123046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5322 - accuracy: 0.9249 - val_loss: 1.2756 - val_accuracy: 0.7510\n",
            "Epoch  16\n",
            "0.0205108251953125\n",
            "0.119842431640625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5316 - accuracy: 0.9253 - val_loss: 1.4673 - val_accuracy: 0.6940\n",
            "Epoch  17\n",
            "0.0488985693359375\n",
            "0.1520788818359375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5219 - accuracy: 0.9272 - val_loss: 1.3719 - val_accuracy: 0.7275\n",
            "Epoch  18\n",
            "0.03233650146484375\n",
            "0.145993603515625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5123 - accuracy: 0.9323 - val_loss: 1.1578 - val_accuracy: 0.7628\n",
            "Epoch  19\n",
            "0.006997203369140625\n",
            "0.112322021484375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5069 - accuracy: 0.9341 - val_loss: 1.3151 - val_accuracy: 0.7482\n",
            "Epoch  20\n",
            "0.018761484375\n",
            "0.13207088623046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4999 - accuracy: 0.9361 - val_loss: 1.3212 - val_accuracy: 0.7361\n",
            "Epoch  21\n",
            "0.03023696533203125\n",
            "0.13717940673828125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.5044 - accuracy: 0.9347 - val_loss: 1.2807 - val_accuracy: 0.7541\n",
            "Epoch  22\n",
            "0.02455224853515625\n",
            "0.137071533203125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4928 - accuracy: 0.9390 - val_loss: 1.1977 - val_accuracy: 0.7524\n",
            "Epoch  23\n",
            "0.011392484130859375\n",
            "0.1205200927734375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4937 - accuracy: 0.9391 - val_loss: 1.2521 - val_accuracy: 0.7674\n",
            "Epoch  24\n",
            "0.01103616455078125\n",
            "0.13252708740234376\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4885 - accuracy: 0.9423 - val_loss: 2.2096 - val_accuracy: 0.6322\n",
            "Epoch  25\n",
            "0.1574427734375\n",
            "0.2553260009765625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4817 - accuracy: 0.9431 - val_loss: 1.7017 - val_accuracy: 0.7094\n",
            "Epoch  26\n",
            "0.0788250634765625\n",
            "0.189942333984375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4856 - accuracy: 0.9422 - val_loss: 1.2390 - val_accuracy: 0.7768\n",
            "Epoch  27\n",
            "0.012935489501953125\n",
            "0.1284528076171875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4732 - accuracy: 0.9477 - val_loss: 1.3185 - val_accuracy: 0.7462\n",
            "Epoch  28\n",
            "0.0289895458984375\n",
            "0.144492529296875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4735 - accuracy: 0.9471 - val_loss: 1.2309 - val_accuracy: 0.7712\n",
            "Epoch  29\n",
            "0.0082412548828125\n",
            "0.127142138671875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4741 - accuracy: 0.9467 - val_loss: 1.3799 - val_accuracy: 0.7405\n",
            "Epoch  30\n",
            "0.0314792431640625\n",
            "0.147394189453125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4715 - accuracy: 0.9471 - val_loss: 1.2504 - val_accuracy: 0.7626\n",
            "Epoch  31\n",
            "0.013769273681640624\n",
            "0.133581884765625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4633 - accuracy: 0.9502 - val_loss: 1.2682 - val_accuracy: 0.7663\n",
            "Epoch  32\n",
            "0.018156976318359375\n",
            "0.130200732421875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4742 - accuracy: 0.9453 - val_loss: 1.6257 - val_accuracy: 0.7331\n",
            "Epoch  33\n",
            "0.06104087890625\n",
            "0.1765620361328125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4662 - accuracy: 0.9490 - val_loss: 1.4272 - val_accuracy: 0.7458\n",
            "Epoch  34\n",
            "0.02685632080078125\n",
            "0.1456336181640625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4551 - accuracy: 0.9526 - val_loss: 1.6522 - val_accuracy: 0.7130\n",
            "Epoch  35\n",
            "0.0586487841796875\n",
            "0.17204947509765625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4603 - accuracy: 0.9502 - val_loss: 1.3584 - val_accuracy: 0.7583\n",
            "Epoch  36\n",
            "0.02055577392578125\n",
            "0.141596923828125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4582 - accuracy: 0.9499 - val_loss: 1.2034 - val_accuracy: 0.7837\n",
            "Epoch  37\n",
            "0.006457969360351563\n",
            "0.12619476318359374\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4540 - accuracy: 0.9521 - val_loss: 1.2628 - val_accuracy: 0.7747\n",
            "Epoch  38\n",
            "0.0156998095703125\n",
            "0.13845811767578126\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4579 - accuracy: 0.9514 - val_loss: 1.2871 - val_accuracy: 0.7415\n",
            "Epoch  39\n",
            "0.019384276123046874\n",
            "0.1315345947265625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4450 - accuracy: 0.9558 - val_loss: 1.3384 - val_accuracy: 0.7430\n",
            "Epoch  40\n",
            "0.02779952880859375\n",
            "0.151411181640625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4481 - accuracy: 0.9532 - val_loss: 1.5804 - val_accuracy: 0.7293\n",
            "Epoch  41\n",
            "0.0527676611328125\n",
            "0.167646533203125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4541 - accuracy: 0.9512 - val_loss: 1.2776 - val_accuracy: 0.7716\n",
            "Epoch  42\n",
            "0.010992496337890624\n",
            "0.1287384521484375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4478 - accuracy: 0.9538 - val_loss: 1.4013 - val_accuracy: 0.7392\n",
            "Epoch  43\n",
            "0.02574527587890625\n",
            "0.15503291015625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4448 - accuracy: 0.9541 - val_loss: 1.0593 - val_accuracy: 0.8046\n",
            "Epoch  44\n",
            "0.009106636962890624\n",
            "0.1018464599609375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4459 - accuracy: 0.9532 - val_loss: 1.3338 - val_accuracy: 0.7461\n",
            "Epoch  45\n",
            "0.01752674560546875\n",
            "0.14145093994140626\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4358 - accuracy: 0.9571 - val_loss: 1.4224 - val_accuracy: 0.7436\n",
            "Epoch  46\n",
            "0.040440341796875\n",
            "0.1624461669921875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4427 - accuracy: 0.9545 - val_loss: 1.2126 - val_accuracy: 0.7777\n",
            "Epoch  47\n",
            "0.010293092041015625\n",
            "0.13297203369140626\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4436 - accuracy: 0.9536 - val_loss: 1.2720 - val_accuracy: 0.7837\n",
            "Epoch  48\n",
            "0.010097598266601563\n",
            "0.1307384033203125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4382 - accuracy: 0.9550 - val_loss: 1.3309 - val_accuracy: 0.7641\n",
            "Epoch  49\n",
            "0.01686499267578125\n",
            "0.140384130859375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4348 - accuracy: 0.9555 - val_loss: 1.3337 - val_accuracy: 0.7634\n",
            "Epoch  50\n",
            "0.015878538818359374\n",
            "0.141324560546875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4334 - accuracy: 0.9569 - val_loss: 1.2540 - val_accuracy: 0.7686\n",
            "Epoch  51\n",
            "0.01748016845703125\n",
            "0.136263330078125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4422 - accuracy: 0.9533 - val_loss: 1.3444 - val_accuracy: 0.7752\n",
            "Epoch  52\n",
            "0.02325154296875\n",
            "0.14172177734375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4308 - accuracy: 0.9574 - val_loss: 1.1437 - val_accuracy: 0.7931\n",
            "Epoch  53\n",
            "0.005181329345703125\n",
            "0.120828662109375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4297 - accuracy: 0.9576 - val_loss: 1.3615 - val_accuracy: 0.7637\n",
            "Epoch  54\n",
            "0.0242584423828125\n",
            "0.1409018798828125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4258 - accuracy: 0.9581 - val_loss: 1.1583 - val_accuracy: 0.7839\n",
            "Epoch  55\n",
            "0.005639732666015625\n",
            "0.124253369140625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4331 - accuracy: 0.9566 - val_loss: 1.2725 - val_accuracy: 0.7856\n",
            "Epoch  56\n",
            "0.01176820556640625\n",
            "0.13534361572265624\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4269 - accuracy: 0.9586 - val_loss: 1.1874 - val_accuracy: 0.7887\n",
            "Epoch  57\n",
            "0.011545048828125\n",
            "0.12786312255859375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4288 - accuracy: 0.9570 - val_loss: 1.1503 - val_accuracy: 0.7909\n",
            "Epoch  58\n",
            "0.005162137451171875\n",
            "0.12208956298828125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4286 - accuracy: 0.9576 - val_loss: 1.2701 - val_accuracy: 0.7776\n",
            "Epoch  59\n",
            "0.01412424560546875\n",
            "0.1380799560546875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4179 - accuracy: 0.9609 - val_loss: 1.3056 - val_accuracy: 0.7688\n",
            "Epoch  60\n",
            "0.01605922119140625\n",
            "0.1383033203125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4221 - accuracy: 0.9588 - val_loss: 1.1622 - val_accuracy: 0.7929\n",
            "Epoch  61\n",
            "0.00431375\n",
            "0.1206915771484375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4226 - accuracy: 0.9583 - val_loss: 1.2352 - val_accuracy: 0.7784\n",
            "Epoch  62\n",
            "0.01914746826171875\n",
            "0.1318209716796875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4179 - accuracy: 0.9595 - val_loss: 1.2118 - val_accuracy: 0.7877\n",
            "Epoch  63\n",
            "0.008814322509765625\n",
            "0.12750504150390626\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4227 - accuracy: 0.9586 - val_loss: 1.3239 - val_accuracy: 0.7759\n",
            "Epoch  64\n",
            "0.020006107177734374\n",
            "0.14270045166015624\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4182 - accuracy: 0.9597 - val_loss: 1.6331 - val_accuracy: 0.7506\n",
            "Epoch  65\n",
            "0.0568513671875\n",
            "0.1657328857421875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4165 - accuracy: 0.9603 - val_loss: 1.1231 - val_accuracy: 0.8014\n",
            "Epoch  66\n",
            "0.003871432189941406\n",
            "0.1129108642578125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4175 - accuracy: 0.9595 - val_loss: 1.4570 - val_accuracy: 0.7520\n",
            "Epoch  67\n",
            "0.044614580078125\n",
            "nan\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4159 - accuracy: 0.9599 - val_loss: 1.2846 - val_accuracy: 0.7777\n",
            "Epoch  68\n",
            "0.013545614013671875\n",
            "0.13768382568359375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4193 - accuracy: 0.9587 - val_loss: 1.3488 - val_accuracy: 0.7700\n",
            "Epoch  69\n",
            "0.02308351806640625\n",
            "0.144970654296875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4131 - accuracy: 0.9600 - val_loss: 1.4519 - val_accuracy: 0.7563\n",
            "Epoch  70\n",
            "0.03403992431640625\n",
            "0.16046312255859374\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4071 - accuracy: 0.9627 - val_loss: 1.2563 - val_accuracy: 0.7962\n",
            "Epoch  71\n",
            "0.0130992578125\n",
            "0.13211171875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4072 - accuracy: 0.9620 - val_loss: 1.4088 - val_accuracy: 0.7680\n",
            "Epoch  72\n",
            "0.02184550048828125\n",
            "0.15415989990234374\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 93s 2ms/sample - loss: 0.4163 - accuracy: 0.9586 - val_loss: 1.3995 - val_accuracy: 0.7491\n",
            "Epoch  73\n",
            "0.03713884033203125\n",
            "0.161944384765625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4079 - accuracy: 0.9615 - val_loss: 1.3361 - val_accuracy: 0.7733\n",
            "Epoch  74\n",
            "0.0344474169921875\n",
            "0.14029447021484376\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 93s 2ms/sample - loss: 0.4046 - accuracy: 0.9616 - val_loss: 1.5932 - val_accuracy: 0.7279\n",
            "Epoch  75\n",
            "0.0621099169921875\n",
            "0.181757275390625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4110 - accuracy: 0.9593 - val_loss: 1.2427 - val_accuracy: 0.7782\n",
            "Epoch  76\n",
            "0.0140137060546875\n",
            "0.131312890625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4064 - accuracy: 0.9616 - val_loss: 1.3155 - val_accuracy: 0.7712\n",
            "Epoch  77\n",
            "0.02397330078125\n",
            "0.1402295654296875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.4038 - accuracy: 0.9624Epoch  78\n",
            "0.0055390716552734375\n",
            "0.119157763671875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4101 - accuracy: 0.9595 - val_loss: 1.5047 - val_accuracy: 0.7527\n",
            "Epoch  79\n",
            "0.0497313134765625\n",
            "0.1625123779296875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4003 - accuracy: 0.9636 - val_loss: 1.2373 - val_accuracy: 0.7888\n",
            "Epoch  80\n",
            "0.0132310986328125\n",
            "0.1322261474609375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4082 - accuracy: 0.9604 - val_loss: 1.1254 - val_accuracy: 0.7881\n",
            "Epoch  81\n",
            "0.004758166809082031\n",
            "0.1216246826171875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.3960 - accuracy: 0.9633 - val_loss: 1.7320 - val_accuracy: 0.7132\n",
            "Epoch  82\n",
            "0.088805791015625\n",
            "0.20093046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.4018 - accuracy: 0.9615 - val_loss: 1.2993 - val_accuracy: 0.7739\n",
            "Epoch  83\n",
            "0.02238492919921875\n",
            "0.1405598876953125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4025 - accuracy: 0.9610 - val_loss: 1.2067 - val_accuracy: 0.7892\n",
            "Epoch  84\n",
            "0.013958321533203125\n",
            "0.12940157470703126\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 94s 2ms/sample - loss: 0.4057 - accuracy: 0.9592 - val_loss: 1.2982 - val_accuracy: 0.7713\n",
            "Epoch  85\n",
            "0.0250651611328125\n",
            "0.14050302734375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3949 - accuracy: 0.9630 - val_loss: 1.4802 - val_accuracy: 0.7477\n",
            "Epoch  86\n",
            "0.0419160546875\n",
            "0.15278212890625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3937 - accuracy: 0.9639 - val_loss: 1.1895 - val_accuracy: 0.7847\n",
            "Epoch  87\n",
            "0.01116828857421875\n",
            "0.127249609375\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3994 - accuracy: 0.9620 - val_loss: 1.3135 - val_accuracy: 0.7686\n",
            "Epoch  88\n",
            "0.02458087646484375\n",
            "0.148948681640625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3984 - accuracy: 0.9620 - val_loss: 1.4782 - val_accuracy: 0.7311\n",
            "Epoch  89\n",
            "0.049510107421875\n",
            "0.16775050048828125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3959 - accuracy: 0.9627 - val_loss: 1.3412 - val_accuracy: 0.7742\n",
            "Epoch  90\n",
            "0.02775532958984375\n",
            "0.14501212158203125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3918 - accuracy: 0.9641 - val_loss: 1.0725 - val_accuracy: 0.8001\n",
            "Epoch  91\n",
            "0.00596772705078125\n",
            "0.10856541748046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3931 - accuracy: 0.9642 - val_loss: 1.3799 - val_accuracy: 0.7719\n",
            "Epoch  92\n",
            "0.026152392578125\n",
            "0.14591300048828126\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3984 - accuracy: 0.9615 - val_loss: 1.4023 - val_accuracy: 0.7665\n",
            "Epoch  93\n",
            "0.03458994140625\n",
            "0.1568044921875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 95s 2ms/sample - loss: 0.3868 - accuracy: 0.9646 - val_loss: 1.4512 - val_accuracy: 0.7593\n",
            "Epoch  94\n",
            "0.03899563720703125\n",
            "0.1620104248046875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 93s 2ms/sample - loss: 0.3935 - accuracy: 0.9616 - val_loss: 1.1980 - val_accuracy: 0.7889\n",
            "Epoch  95\n",
            "0.01387737060546875\n",
            "0.1286733642578125\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 93s 2ms/sample - loss: 0.3930 - accuracy: 0.9624 - val_loss: 1.2365 - val_accuracy: 0.7795\n",
            "Epoch  96\n",
            "0.015222535400390626\n",
            "0.137171435546875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 92s 2ms/sample - loss: 0.3917 - accuracy: 0.9643 - val_loss: 1.2804 - val_accuracy: 0.7867\n",
            "Epoch  97\n",
            "0.01670895751953125\n",
            "0.13651025390625\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 93s 2ms/sample - loss: 0.3943 - accuracy: 0.9619 - val_loss: 1.0682 - val_accuracy: 0.8094\n",
            "Epoch  98\n",
            "0.003128482971191406\n",
            "0.1080024169921875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 92s 2ms/sample - loss: 0.3947 - accuracy: 0.9620 - val_loss: 1.2079 - val_accuracy: 0.7877\n",
            "Epoch  99\n",
            "0.012502142333984376\n",
            "0.13291982421875\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "50000/50000 [==============================] - 92s 2ms/sample - loss: 0.3820 - accuracy: 0.9657 - val_loss: 1.0751 - val_accuracy: 0.8046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFNV6qf-uExF",
        "colab_type": "code",
        "outputId": "83e72731-7006-4b97-b647-e6e000871926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(test_ece))\n",
        "print(len(np.mean(test_ece_list,axis=0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk_CMK2Ou98v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ece_list.append(test_ece)\n",
        "train_ece_list.append(train_ece)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvns5R3LupPC",
        "colab_type": "code",
        "outputId": "4157e740-e96b-4840-b3fa-018b04681ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "plt.plot(range(1,100), np.nanmean(test_ece_list,axis=0)[1:], '-ok');\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD1CAYAAABA+A6aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQELhRTSvFPs",
        "colab_type": "code",
        "outputId": "7a82591a-aa9b-423f-a9c2-c8fe6c338529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "plt.plot(range(1,100), np.nanmean(train_ece_list,axis=0)[1:], '-ok');\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD3CAYAAAANMK+RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7aG5ny723Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('test_ece_list',np.array(test_ece_list))\n",
        "np.save('train_ece_list',np.array(train_ece_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBhHbxah3Nsv",
        "colab_type": "code",
        "outputId": "70179efb-c271-4ef3-e4d4-bf08328049e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bin\t datalab  home\t lib64\topt   run   swift\t\ttmp    var\n",
            "boot\t dev\t  lib\t media\tproc  sbin  sys\t\t\ttools\n",
            "content  etc\t  lib32  mnt\troot  srv   tensorflow-2.0.0b1\tusr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDkq-coWpURa",
        "colab_type": "code",
        "outputId": "6acf45b4-29e4-4fde-d0a8-b0d13c5ead96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "#callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
        "\n",
        "# Run training, with or without data augmentation.\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        featurewise_center=False,\n",
        "        # set each sample mean to 0\n",
        "        samplewise_center=False,\n",
        "        # divide inputs by std of dataset\n",
        "        featurewise_std_normalization=False,\n",
        "        # divide each input by its std\n",
        "        samplewise_std_normalization=False,\n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=0,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.1,\n",
        "        # set range for random shear\n",
        "        shear_range=0.,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        "\n",
        "    # Compute quantities required for featurewise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs, verbose=1, workers=4,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   272         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 32, 32, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1040        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 64)   1088        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16, 16, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 16, 16, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 64)   8256        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 128)  8320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 64)   8256        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 128)  8320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 128)    16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 128)    512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 8, 8, 128)    0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    33024       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 8, 8, 256)    0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 128)    32896       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 128)    512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    147584      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 256)    33024       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 256)    0           add_6[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 128)    32896       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 128)    512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 128)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 256)    33024       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 256)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 1, 1, 256)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 256)          0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 849,002\n",
            "Trainable params: 843,786\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n",
            "ResNet29v2\n",
            "Not using data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 04:08:32.034775 139983337224064 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "50000/50000 [==============================] - 64s 1ms/sample - loss: 1.8356 - accuracy: 0.5094 - val_loss: 1.5700 - val_accuracy: 0.5896\n",
            "Epoch 2/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 1.3309 - accuracy: 0.6582 - val_loss: 1.4799 - val_accuracy: 0.5951\n",
            "Epoch 3/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 1.1114 - accuracy: 0.7254 - val_loss: 1.3418 - val_accuracy: 0.6671\n",
            "Epoch 4/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.9785 - accuracy: 0.7675 - val_loss: 1.1943 - val_accuracy: 0.6923\n",
            "Epoch 5/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.8877 - accuracy: 0.7963 - val_loss: 1.2010 - val_accuracy: 0.6915\n",
            "Epoch 6/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.8152 - accuracy: 0.8210 - val_loss: 1.2800 - val_accuracy: 0.6758\n",
            "Epoch 7/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.7624 - accuracy: 0.8390 - val_loss: 1.1129 - val_accuracy: 0.7365\n",
            "Epoch 8/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.7123 - accuracy: 0.8578 - val_loss: 1.1466 - val_accuracy: 0.7301\n",
            "Epoch 9/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.6805 - accuracy: 0.8672 - val_loss: 1.2512 - val_accuracy: 0.7204\n",
            "Epoch 10/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.6426 - accuracy: 0.8812 - val_loss: 1.3578 - val_accuracy: 0.6952\n",
            "Epoch 11/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.6137 - accuracy: 0.8914 - val_loss: 1.3505 - val_accuracy: 0.6888\n",
            "Epoch 12/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5951 - accuracy: 0.8990 - val_loss: 1.0760 - val_accuracy: 0.7526\n",
            "Epoch 13/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5765 - accuracy: 0.9075 - val_loss: 1.7970 - val_accuracy: 0.6450\n",
            "Epoch 14/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5622 - accuracy: 0.9130 - val_loss: 1.1717 - val_accuracy: 0.7446\n",
            "Epoch 15/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5515 - accuracy: 0.9180 - val_loss: 1.2965 - val_accuracy: 0.7398\n",
            "Epoch 16/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5376 - accuracy: 0.9228 - val_loss: 1.5065 - val_accuracy: 0.7186\n",
            "Epoch 17/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5262 - accuracy: 0.9271 - val_loss: 1.4520 - val_accuracy: 0.7251\n",
            "Epoch 18/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5157 - accuracy: 0.9312 - val_loss: 1.8407 - val_accuracy: 0.6677\n",
            "Epoch 19/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5187 - accuracy: 0.9294 - val_loss: 1.1023 - val_accuracy: 0.7702\n",
            "Epoch 20/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5070 - accuracy: 0.9352 - val_loss: 1.2354 - val_accuracy: 0.7497\n",
            "Epoch 21/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5019 - accuracy: 0.9377 - val_loss: 1.2807 - val_accuracy: 0.7468\n",
            "Epoch 22/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.5008 - accuracy: 0.9367 - val_loss: 1.4337 - val_accuracy: 0.7216\n",
            "Epoch 23/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4903 - accuracy: 0.9410 - val_loss: 1.0655 - val_accuracy: 0.7851\n",
            "Epoch 24/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4928 - accuracy: 0.9398 - val_loss: 1.5272 - val_accuracy: 0.7065\n",
            "Epoch 25/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4843 - accuracy: 0.9429 - val_loss: 1.3663 - val_accuracy: 0.7577\n",
            "Epoch 26/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4856 - accuracy: 0.9430 - val_loss: 1.7437 - val_accuracy: 0.7169\n",
            "Epoch 27/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4853 - accuracy: 0.9419 - val_loss: 1.2582 - val_accuracy: 0.7620\n",
            "Epoch 28/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4791 - accuracy: 0.9445 - val_loss: 1.3127 - val_accuracy: 0.7573\n",
            "Epoch 29/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4756 - accuracy: 0.9461 - val_loss: 1.4770 - val_accuracy: 0.7404\n",
            "Epoch 30/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4714 - accuracy: 0.9466 - val_loss: 1.2701 - val_accuracy: 0.7496\n",
            "Epoch 31/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4724 - accuracy: 0.9474 - val_loss: 1.2332 - val_accuracy: 0.7633\n",
            "Epoch 32/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4652 - accuracy: 0.9492 - val_loss: 1.5170 - val_accuracy: 0.7388\n",
            "Epoch 33/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4689 - accuracy: 0.9484 - val_loss: 1.2137 - val_accuracy: 0.7742\n",
            "Epoch 34/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4676 - accuracy: 0.9482 - val_loss: 1.2769 - val_accuracy: 0.7609\n",
            "Epoch 35/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4551 - accuracy: 0.9527 - val_loss: 1.1639 - val_accuracy: 0.7945\n",
            "Epoch 36/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4581 - accuracy: 0.9514 - val_loss: 1.4228 - val_accuracy: 0.7365\n",
            "Epoch 37/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4668 - accuracy: 0.9485 - val_loss: 1.5420 - val_accuracy: 0.7102\n",
            "Epoch 38/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4620 - accuracy: 0.9499 - val_loss: 1.3636 - val_accuracy: 0.7585\n",
            "Epoch 39/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4480 - accuracy: 0.9549 - val_loss: 1.5358 - val_accuracy: 0.7118\n",
            "Epoch 40/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4572 - accuracy: 0.9511 - val_loss: 1.3051 - val_accuracy: 0.7609\n",
            "Epoch 41/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4469 - accuracy: 0.9539 - val_loss: 1.4306 - val_accuracy: 0.7563\n",
            "Epoch 42/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4479 - accuracy: 0.9546 - val_loss: 1.2456 - val_accuracy: 0.7667\n",
            "Epoch 43/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4460 - accuracy: 0.9540 - val_loss: 1.3996 - val_accuracy: 0.7687\n",
            "Epoch 44/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4440 - accuracy: 0.9551 - val_loss: 1.5157 - val_accuracy: 0.7566\n",
            "Epoch 45/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4424 - accuracy: 0.9549 - val_loss: 1.8228 - val_accuracy: 0.7359\n",
            "Epoch 46/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4493 - accuracy: 0.9531 - val_loss: 1.3439 - val_accuracy: 0.7738\n",
            "Epoch 47/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4408 - accuracy: 0.9557 - val_loss: 1.1576 - val_accuracy: 0.7991\n",
            "Epoch 48/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4451 - accuracy: 0.9542 - val_loss: 1.1592 - val_accuracy: 0.7782\n",
            "Epoch 49/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4358 - accuracy: 0.9564 - val_loss: 1.2981 - val_accuracy: 0.7708\n",
            "Epoch 50/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4426 - accuracy: 0.9539 - val_loss: 1.1532 - val_accuracy: 0.7846\n",
            "Epoch 51/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4338 - accuracy: 0.9569 - val_loss: 1.4397 - val_accuracy: 0.7464\n",
            "Epoch 52/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4328 - accuracy: 0.9573 - val_loss: 1.4030 - val_accuracy: 0.7563\n",
            "Epoch 53/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4361 - accuracy: 0.9557 - val_loss: 1.3467 - val_accuracy: 0.7631\n",
            "Epoch 54/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4324 - accuracy: 0.9575 - val_loss: 1.3465 - val_accuracy: 0.7661\n",
            "Epoch 55/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4313 - accuracy: 0.9579 - val_loss: 1.1053 - val_accuracy: 0.7942\n",
            "Epoch 56/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4285 - accuracy: 0.9573 - val_loss: 1.3294 - val_accuracy: 0.7720\n",
            "Epoch 57/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4332 - accuracy: 0.9561 - val_loss: 1.5504 - val_accuracy: 0.7227\n",
            "Epoch 58/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4288 - accuracy: 0.9581 - val_loss: 1.2873 - val_accuracy: 0.7766\n",
            "Epoch 59/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4279 - accuracy: 0.9585 - val_loss: 1.2690 - val_accuracy: 0.7747\n",
            "Epoch 60/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4222 - accuracy: 0.9589 - val_loss: 1.2774 - val_accuracy: 0.7750\n",
            "Epoch 61/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4299 - accuracy: 0.9571 - val_loss: 1.2781 - val_accuracy: 0.7724\n",
            "Epoch 62/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4240 - accuracy: 0.9587 - val_loss: 1.4700 - val_accuracy: 0.7453\n",
            "Epoch 63/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4252 - accuracy: 0.9584 - val_loss: 1.4111 - val_accuracy: 0.7741\n",
            "Epoch 64/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4202 - accuracy: 0.9585 - val_loss: 1.6677 - val_accuracy: 0.7175\n",
            "Epoch 65/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4171 - accuracy: 0.9600 - val_loss: 1.2697 - val_accuracy: 0.7781\n",
            "Epoch 66/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4192 - accuracy: 0.9598 - val_loss: 1.2559 - val_accuracy: 0.7865\n",
            "Epoch 67/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4229 - accuracy: 0.9575 - val_loss: 1.4243 - val_accuracy: 0.7730\n",
            "Epoch 68/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4142 - accuracy: 0.9600 - val_loss: 1.2572 - val_accuracy: 0.7650\n",
            "Epoch 69/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4164 - accuracy: 0.9593 - val_loss: 1.2352 - val_accuracy: 0.7974\n",
            "Epoch 70/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4150 - accuracy: 0.9596 - val_loss: 1.2405 - val_accuracy: 0.7980\n",
            "Epoch 71/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4136 - accuracy: 0.9604 - val_loss: 1.3814 - val_accuracy: 0.7724\n",
            "Epoch 72/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4155 - accuracy: 0.9598 - val_loss: 1.1797 - val_accuracy: 0.7928\n",
            "Epoch 73/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4102 - accuracy: 0.9604 - val_loss: 1.4621 - val_accuracy: 0.7518\n",
            "Epoch 74/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4067 - accuracy: 0.9620 - val_loss: 1.5687 - val_accuracy: 0.7596\n",
            "Epoch 75/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4115 - accuracy: 0.9601 - val_loss: 1.4929 - val_accuracy: 0.7400\n",
            "Epoch 76/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4104 - accuracy: 0.9597 - val_loss: 1.2926 - val_accuracy: 0.7759\n",
            "Epoch 77/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4063 - accuracy: 0.9625 - val_loss: 1.2352 - val_accuracy: 0.7916\n",
            "Epoch 78/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4127 - accuracy: 0.9599 - val_loss: 1.1882 - val_accuracy: 0.7951\n",
            "Epoch 79/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4021 - accuracy: 0.9631 - val_loss: 1.4732 - val_accuracy: 0.7489\n",
            "Epoch 80/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4068 - accuracy: 0.9605 - val_loss: 1.1992 - val_accuracy: 0.7785\n",
            "Epoch 81/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4048 - accuracy: 0.9613 - val_loss: 1.2175 - val_accuracy: 0.7939\n",
            "Epoch 82/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3966 - accuracy: 0.9635 - val_loss: 1.0623 - val_accuracy: 0.7989\n",
            "Epoch 83/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4067 - accuracy: 0.9602 - val_loss: 1.4912 - val_accuracy: 0.7570\n",
            "Epoch 84/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4013 - accuracy: 0.9611 - val_loss: 1.3353 - val_accuracy: 0.7471\n",
            "Epoch 85/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.4056 - accuracy: 0.9600 - val_loss: 1.1065 - val_accuracy: 0.7904\n",
            "Epoch 86/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3939 - accuracy: 0.9640 - val_loss: 1.1724 - val_accuracy: 0.7914\n",
            "Epoch 87/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.4044 - accuracy: 0.9602 - val_loss: 1.4570 - val_accuracy: 0.7341\n",
            "Epoch 88/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3941 - accuracy: 0.9630 - val_loss: 1.2661 - val_accuracy: 0.7746\n",
            "Epoch 89/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3984 - accuracy: 0.9614 - val_loss: 1.2097 - val_accuracy: 0.7874\n",
            "Epoch 90/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3971 - accuracy: 0.9627 - val_loss: 1.1125 - val_accuracy: 0.8047\n",
            "Epoch 91/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3981 - accuracy: 0.9626 - val_loss: 1.2681 - val_accuracy: 0.7789\n",
            "Epoch 92/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3917 - accuracy: 0.9641 - val_loss: 1.3833 - val_accuracy: 0.7716\n",
            "Epoch 93/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3959 - accuracy: 0.9620 - val_loss: 1.2831 - val_accuracy: 0.7692\n",
            "Epoch 94/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3937 - accuracy: 0.9623 - val_loss: 1.2634 - val_accuracy: 0.7856\n",
            "Epoch 95/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3957 - accuracy: 0.9622 - val_loss: 1.2806 - val_accuracy: 0.7745\n",
            "Epoch 96/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3889 - accuracy: 0.9641 - val_loss: 1.4043 - val_accuracy: 0.7578\n",
            "Epoch 97/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3941 - accuracy: 0.9621 - val_loss: 1.3250 - val_accuracy: 0.7744\n",
            "Epoch 98/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3860 - accuracy: 0.9653 - val_loss: 1.1968 - val_accuracy: 0.7993\n",
            "Epoch 99/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3899 - accuracy: 0.9627 - val_loss: 1.4283 - val_accuracy: 0.7646\n",
            "Epoch 100/200\n",
            "50000/50000 [==============================] - 53s 1ms/sample - loss: 0.3952 - accuracy: 0.9617 - val_loss: 1.1014 - val_accuracy: 0.8088\n",
            "Epoch 101/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3869 - accuracy: 0.9643 - val_loss: 1.2036 - val_accuracy: 0.7944\n",
            "Epoch 102/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3869 - accuracy: 0.9637 - val_loss: 1.3105 - val_accuracy: 0.7720\n",
            "Epoch 103/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3865 - accuracy: 0.9638 - val_loss: 1.1166 - val_accuracy: 0.8049\n",
            "Epoch 104/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3835 - accuracy: 0.9642 - val_loss: 1.2574 - val_accuracy: 0.7740\n",
            "Epoch 105/200\n",
            "50000/50000 [==============================] - 51s 1ms/sample - loss: 0.3862 - accuracy: 0.9634 - val_loss: 1.0905 - val_accuracy: 0.8109\n",
            "Epoch 106/200\n",
            "50000/50000 [==============================] - 52s 1ms/sample - loss: 0.3870 - accuracy: 0.9622 - val_loss: 1.1019 - val_accuracy: 0.8014\n",
            "Epoch 107/200\n",
            "44864/50000 [=========================>....] - ETA: 5s - loss: 0.3802 - accuracy: 0.9657"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-72b7881d1688>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m               shuffle=True)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using real-time data augmentation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCdL1mVnp0BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ECE_correct(probs,correct):\n",
        "  #correct are 0 or 1 for incorrect or correct prediction\n",
        "  #probs are highest prob for each example\n",
        "  #probably also want to look at more than just highest later and see how good those are too\n",
        "  #tf.print(probs.shape,correct.shape)\n",
        "  #start with 10 bins\n",
        "  num_bins=50\n",
        "  inds=tf.argsort(probs,axis=-1)\n",
        "  probs=tf.gather(probs,inds)\n",
        "  correct=tf.gather(correct,inds)\n",
        "  #tf.print('probs',probs)\n",
        "  #tf.print('correct',correct)\n",
        "  inds=[]\n",
        "  current=int(num_bins*.3)\n",
        "  for i in range(len(probs)):\n",
        "    if probs[i]>current/float(num_bins):\n",
        "      inds.append(i)\n",
        "      current+=1\n",
        "      #print(current)\n",
        "      #print(probs[i])\n",
        "    if current==num_bins: break\n",
        "      \n",
        "  #print('inds',inds)\n",
        "  sizes=[]\n",
        "  total=0\n",
        "  for i in range(len(inds)):\n",
        "    if i==0: sizes.append(inds[i])\n",
        "    else: sizes.append(inds[i]-inds[i-1])\n",
        "  sizes.append(len(probs)-inds[-1])\n",
        "  \n",
        "  #print('sizes',sizes)\n",
        "      \n",
        "  prob_splits=tf.split(probs,sizes)\n",
        "  correct_splits=tf.split(correct,sizes)\n",
        "  #tf.print('prob_splits[0]',prob_splits[0])\n",
        "  #tf.print('correct_splits[0]',correct_splits[0])\n",
        "  \n",
        "  mean_probs=[tf.reduce_mean(split) for split in prob_splits]\n",
        "  mean_acc=[tf.reduce_mean(split) for split in correct_splits]\n",
        "  \n",
        "  #tf.print('mean_probs',mean_probs)\n",
        "  #tf.print('mean_acc',mean_acc)\n",
        "  #tf.print(mean_acc[0])\n",
        "  #a=mean_acc[0]-mean_probs[0]\n",
        "  \n",
        "  weighted_errors=[sizes[i]*abs(mean_acc[i]-mean_probs[i]) for i in range(len(sizes))]\n",
        "  \n",
        "  ece=np.sum(weighted_errors)/float(len(probs))\n",
        "  return ece\n",
        "\n",
        "def ECE(pred,y_test):\n",
        "  #input are raw probabilities and test data\n",
        "  max_probs=pred[np.arange(len(pred)),np.argmax(pred,axis=-1)]\n",
        "  classes=np.argmax(y_test,axis=-1)\n",
        "  correct=np.equal(np.argmax(pred,axis=-1),classes)\n",
        "  return ECE_correct(tf.cast(max_probs,tf.float32),tf.cast(correct,tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWkG4pvB_le-",
        "colab_type": "code",
        "outputId": "0cc53797-1028-4aaf-aa9a-74b657dc6d74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pred=model.predict(x_test)\n",
        "print(model.evaluate(x_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 395us/sample - loss: 1.4152 - accuracy: 0.7629\n",
            "[1.4152463618278504, 0.7629]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCB8EiVB_yZ0",
        "colab_type": "code",
        "outputId": "31ba8ec8-4c16-4e0e-a57a-42f72ce16ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(np.argmax(pred,axis=-1)[:20])\n",
        "print(pred[np.arange(len(pred)),np.argmax(pred,axis=-1)][:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 1 1 1 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]\n",
            "[0.89591116 0.99982846 0.7993118  0.53371274 0.99942756 0.9002451\n",
            " 0.9999968  0.8814435  0.9995988  0.99999356 0.9999893  1.\n",
            " 0.9835624  0.9999808  0.9999912  0.84776574 0.99999905 0.5998045\n",
            " 0.9687728  1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEFcoYj_6WZ",
        "colab_type": "code",
        "outputId": "dc461051-2690-4791-f53a-3575d1fe518c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(np.argmax(y_test,axis=-1)[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 8 8 0 6 6 1 6 3 1 0 9 5 7 9 8 5 7 8 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvvJWKOQAkqP",
        "colab_type": "code",
        "outputId": "e28ad802-691a-4584-81e8-a6569b670e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(np.equal(np.argmax(pred,axis=-1),np.argmax(y_test,axis=-1))[:20])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ True False False False  True  True  True  True  True  True  True  True\n",
            "  True  True  True  True  True  True  True  True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unc9_WuYA5Gg",
        "colab_type": "code",
        "outputId": "a386cbae-7b45-4c1b-aa0e-6bdd8b00c157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean((np.equal(np.argmax(pred,axis=-1),np.argmax(y_test,axis=-1))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UpKD3csBAWi",
        "colab_type": "code",
        "outputId": "9f6d575e-71f3-4e24-f07d-a3f477615564",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "ECE(pred,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorShape([10000]) TensorShape([10000])\n",
            "probs [0.253340214 0.26166755 0.273774981 ... 1 1 1]\n",
            "correct [0 0 0 ... 1 1 1]\n",
            "4\n",
            "tf.Tensor(0.30788773, shape=(), dtype=float32)\n",
            "5\n",
            "tf.Tensor(0.40141052, shape=(), dtype=float32)\n",
            "6\n",
            "tf.Tensor(0.5002312, shape=(), dtype=float32)\n",
            "7\n",
            "tf.Tensor(0.60030997, shape=(), dtype=float32)\n",
            "8\n",
            "tf.Tensor(0.7002798, shape=(), dtype=float32)\n",
            "9\n",
            "tf.Tensor(0.8001268, shape=(), dtype=float32)\n",
            "10\n",
            "tf.Tensor(0.9002451, shape=(), dtype=float32)\n",
            "inds [6, 64, 271, 761, 1201, 1775, 2507]\n",
            "sizes [6, 58, 207, 490, 440, 574, 732, 7493]\n",
            "prob_splits[0] [0.253340214 0.26166755 0.273774981 0.288490653 0.291321278 0.299973071]\n",
            "correct_splits[0] [0 0 0 0 0 0]\n",
            "mean_probs [0.27809462, 0.369045496, 0.457514673, 0.549990773, 0.651206672, 0.751099288, 0.852908432, 0.98860693]\n",
            "mean_acc [0, 0.310344815, 0.299516916, 0.397959173, 0.436363637, 0.484320551, 0.558743179, 0.864139855]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1507896728515625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RC28qMeBto9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train=model.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxpcnftDJ4a4",
        "colab_type": "code",
        "outputId": "96a798bb-b2ab-4eac-c24f-3fdb7a2f31d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "ECE(pred_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorShape([50000]) TensorShape([50000])\n",
            "probs [0.256432384 0.2702142 0.273556948 ... 1 1 1]\n",
            "correct [0 0 0 ... 1 1 1]\n",
            "4\n",
            "tf.Tensor(0.3006269, shape=(), dtype=float32)\n",
            "5\n",
            "tf.Tensor(0.40010098, shape=(), dtype=float32)\n",
            "6\n",
            "tf.Tensor(0.5000226, shape=(), dtype=float32)\n",
            "7\n",
            "tf.Tensor(0.6000785, shape=(), dtype=float32)\n",
            "8\n",
            "tf.Tensor(0.7000573, shape=(), dtype=float32)\n",
            "9\n",
            "tf.Tensor(0.8002266, shape=(), dtype=float32)\n",
            "10\n",
            "tf.Tensor(0.90001076, shape=(), dtype=float32)\n",
            "inds [12, 195, 813, 2272, 3830, 5737, 8531]\n",
            "sizes [12, 183, 618, 1459, 1558, 1907, 2794, 41469]\n",
            "prob_splits[0] [0.256432384 0.2702142 0.273556948 ... 0.294359118 0.295161813 0.298843801]\n",
            "correct_splits[0] [0 0 0 ... 0 0 0]\n",
            "mean_probs [0.282539815, 0.361180365, 0.461114734, 0.548832834, 0.651640773, 0.751428604, 0.854699373, 0.99107]\n",
            "mean_acc [0.0833333358, 0.360655725, 0.41747573, 0.502398908, 0.579589188, 0.65600419, 0.739799559, 0.971810281]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03022286865234375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMWzHa0KJ_x1",
        "colab_type": "code",
        "outputId": "ecfba34c-d2d9-45f0-afd4-747ff67c1289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ECE(pred,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorShape([10000]) TensorShape([10000])\n",
            "probs [0.253340214 0.26166755 0.273774981 ... 1 1 1]\n",
            "correct [0 0 0 ... 1 1 1]\n",
            "16\n",
            "tf.Tensor(0.30788773, shape=(), dtype=float32)\n",
            "17\n",
            "tf.Tensor(0.3206472, shape=(), dtype=float32)\n",
            "18\n",
            "tf.Tensor(0.34165972, shape=(), dtype=float32)\n",
            "19\n",
            "tf.Tensor(0.36061415, shape=(), dtype=float32)\n",
            "20\n",
            "tf.Tensor(0.380566, shape=(), dtype=float32)\n",
            "21\n",
            "tf.Tensor(0.40141052, shape=(), dtype=float32)\n",
            "22\n",
            "tf.Tensor(0.420108, shape=(), dtype=float32)\n",
            "23\n",
            "tf.Tensor(0.4407618, shape=(), dtype=float32)\n",
            "24\n",
            "tf.Tensor(0.460407, shape=(), dtype=float32)\n",
            "25\n",
            "tf.Tensor(0.4804753, shape=(), dtype=float32)\n",
            "26\n",
            "tf.Tensor(0.5002312, shape=(), dtype=float32)\n",
            "27\n",
            "tf.Tensor(0.52020264, shape=(), dtype=float32)\n",
            "28\n",
            "tf.Tensor(0.54000777, shape=(), dtype=float32)\n",
            "29\n",
            "tf.Tensor(0.56040895, shape=(), dtype=float32)\n",
            "30\n",
            "tf.Tensor(0.5801127, shape=(), dtype=float32)\n",
            "31\n",
            "tf.Tensor(0.60030997, shape=(), dtype=float32)\n",
            "32\n",
            "tf.Tensor(0.6202526, shape=(), dtype=float32)\n",
            "33\n",
            "tf.Tensor(0.6401434, shape=(), dtype=float32)\n",
            "34\n",
            "tf.Tensor(0.6602577, shape=(), dtype=float32)\n",
            "35\n",
            "tf.Tensor(0.68045247, shape=(), dtype=float32)\n",
            "36\n",
            "tf.Tensor(0.7002798, shape=(), dtype=float32)\n",
            "37\n",
            "tf.Tensor(0.72003144, shape=(), dtype=float32)\n",
            "38\n",
            "tf.Tensor(0.74004567, shape=(), dtype=float32)\n",
            "39\n",
            "tf.Tensor(0.76007354, shape=(), dtype=float32)\n",
            "40\n",
            "tf.Tensor(0.7801646, shape=(), dtype=float32)\n",
            "41\n",
            "tf.Tensor(0.8001268, shape=(), dtype=float32)\n",
            "42\n",
            "tf.Tensor(0.8201049, shape=(), dtype=float32)\n",
            "43\n",
            "tf.Tensor(0.8400846, shape=(), dtype=float32)\n",
            "44\n",
            "tf.Tensor(0.86032754, shape=(), dtype=float32)\n",
            "45\n",
            "tf.Tensor(0.88010764, shape=(), dtype=float32)\n",
            "46\n",
            "tf.Tensor(0.9002451, shape=(), dtype=float32)\n",
            "47\n",
            "tf.Tensor(0.92012215, shape=(), dtype=float32)\n",
            "48\n",
            "tf.Tensor(0.94001675, shape=(), dtype=float32)\n",
            "49\n",
            "tf.Tensor(0.9600206, shape=(), dtype=float32)\n",
            "50\n",
            "tf.Tensor(0.98002577, shape=(), dtype=float32)\n",
            "inds [6, 8, 14, 27, 39, 64, 92, 123, 159, 225, 271, 369, 475, 566, 659, 761, 847, 921, 1014, 1115, 1201, 1316, 1425, 1534, 1644, 1775, 1899, 2027, 2190, 2342, 2507, 2726, 2999, 3328, 3879]\n",
            "sizes [6, 2, 6, 13, 12, 25, 28, 31, 36, 66, 46, 98, 106, 91, 93, 102, 86, 74, 93, 101, 86, 115, 109, 109, 110, 131, 124, 128, 163, 152, 165, 219, 273, 329, 551, 6121]\n",
            "prob_splits[0] [0.253340214 0.26166755 0.273774981 0.288490653 0.291321278 0.299973071]\n",
            "correct_splits[0] [0 0 0 0 0 0]\n",
            "mean_probs [0.27809462,\n",
            " 0.310077131,\n",
            " 0.330366731,\n",
            " 0.351878345,\n",
            " 0.371411651,\n",
            " 0.390836984,\n",
            " 0.411103934,\n",
            " 0.4300583,\n",
            " 0.451112062,\n",
            " 0.469965458,\n",
            " 0.491414309,\n",
            " 0.51062113,\n",
            " 0.531273663,\n",
            " 0.550201178,\n",
            " 0.569631934,\n",
            " 0.589172,\n",
            " 0.609788537,\n",
            " 0.629892945,\n",
            " 0.650129735,\n",
            " 0.669967294,\n",
            " 0.690096617,\n",
            " 0.709744,\n",
            " 0.730134904,\n",
            " 0.749535263,\n",
            " 0.769878805,\n",
            " 0.790379405,\n",
            " 0.810512662,\n",
            " 0.830000222,\n",
            " 0.849773109,\n",
            " 0.870014489,\n",
            " 0.889879823,\n",
            " 0.909283638,\n",
            " 0.929836929,\n",
            " 0.950735509,\n",
            " 0.970816433,\n",
            " 0.997703195]\n",
            "mean_acc [0,\n",
            " 0.5,\n",
            " 0.333333343,\n",
            " 0.461538464,\n",
            " 0.25,\n",
            " 0.24,\n",
            " 0.321428567,\n",
            " 0.258064508,\n",
            " 0.305555552,\n",
            " 0.318181813,\n",
            " 0.282608688,\n",
            " 0.397959173,\n",
            " 0.330188692,\n",
            " 0.384615391,\n",
            " 0.494623661,\n",
            " 0.392156869,\n",
            " 0.383720934,\n",
            " 0.337837845,\n",
            " 0.45161289,\n",
            " 0.485148519,\n",
            " 0.5,\n",
            " 0.504347801,\n",
            " 0.449541271,\n",
            " 0.513761461,\n",
            " 0.536363661,\n",
            " 0.427480906,\n",
            " 0.524193525,\n",
            " 0.5625,\n",
            " 0.55214721,\n",
            " 0.572368443,\n",
            " 0.575757563,\n",
            " 0.570776284,\n",
            " 0.622710645,\n",
            " 0.671732545,\n",
            " 0.680580735,\n",
            " 0.912269235]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15115433349609375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqKLuhNQKttr",
        "colab_type": "code",
        "outputId": "44225dcd-291d-4872-ee65-3e248b724f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ECE(pred_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorShape([50000]) TensorShape([50000])\n",
            "probs [0.256432384 0.2702142 0.273556948 ... 1 1 1]\n",
            "correct [0 0 0 ... 1 1 1]\n",
            "16\n",
            "tf.Tensor(0.3006269, shape=(), dtype=float32)\n",
            "17\n",
            "tf.Tensor(0.32022187, shape=(), dtype=float32)\n",
            "18\n",
            "tf.Tensor(0.34013262, shape=(), dtype=float32)\n",
            "19\n",
            "tf.Tensor(0.36025882, shape=(), dtype=float32)\n",
            "20\n",
            "tf.Tensor(0.3801027, shape=(), dtype=float32)\n",
            "21\n",
            "tf.Tensor(0.40010098, shape=(), dtype=float32)\n",
            "22\n",
            "tf.Tensor(0.42011607, shape=(), dtype=float32)\n",
            "23\n",
            "tf.Tensor(0.44037503, shape=(), dtype=float32)\n",
            "24\n",
            "tf.Tensor(0.46059698, shape=(), dtype=float32)\n",
            "25\n",
            "tf.Tensor(0.4802546, shape=(), dtype=float32)\n",
            "26\n",
            "tf.Tensor(0.5000226, shape=(), dtype=float32)\n",
            "27\n",
            "tf.Tensor(0.5200013, shape=(), dtype=float32)\n",
            "28\n",
            "tf.Tensor(0.5400108, shape=(), dtype=float32)\n",
            "29\n",
            "tf.Tensor(0.5600885, shape=(), dtype=float32)\n",
            "30\n",
            "tf.Tensor(0.5800142, shape=(), dtype=float32)\n",
            "31\n",
            "tf.Tensor(0.6000785, shape=(), dtype=float32)\n",
            "32\n",
            "tf.Tensor(0.62001616, shape=(), dtype=float32)\n",
            "33\n",
            "tf.Tensor(0.64005387, shape=(), dtype=float32)\n",
            "34\n",
            "tf.Tensor(0.6600299, shape=(), dtype=float32)\n",
            "35\n",
            "tf.Tensor(0.6800258, shape=(), dtype=float32)\n",
            "36\n",
            "tf.Tensor(0.7000573, shape=(), dtype=float32)\n",
            "37\n",
            "tf.Tensor(0.720007, shape=(), dtype=float32)\n",
            "38\n",
            "tf.Tensor(0.7400142, shape=(), dtype=float32)\n",
            "39\n",
            "tf.Tensor(0.7600257, shape=(), dtype=float32)\n",
            "40\n",
            "tf.Tensor(0.7800297, shape=(), dtype=float32)\n",
            "41\n",
            "tf.Tensor(0.8002266, shape=(), dtype=float32)\n",
            "42\n",
            "tf.Tensor(0.8200071, shape=(), dtype=float32)\n",
            "43\n",
            "tf.Tensor(0.8400259, shape=(), dtype=float32)\n",
            "44\n",
            "tf.Tensor(0.860018, shape=(), dtype=float32)\n",
            "45\n",
            "tf.Tensor(0.8800062, shape=(), dtype=float32)\n",
            "46\n",
            "tf.Tensor(0.90001076, shape=(), dtype=float32)\n",
            "47\n",
            "tf.Tensor(0.9200229, shape=(), dtype=float32)\n",
            "48\n",
            "tf.Tensor(0.9400347, shape=(), dtype=float32)\n",
            "49\n",
            "tf.Tensor(0.96000487, shape=(), dtype=float32)\n",
            "50\n",
            "tf.Tensor(0.98000103, shape=(), dtype=float32)\n",
            "inds [12, 30, 51, 89, 141, 195, 259, 351, 470, 611, 813, 1139, 1418, 1704, 1998, 2272, 2560, 2865, 3187, 3470, 3830, 4173, 4565, 4937, 5324, 5737, 6219, 6693, 7200, 7816, 8531, 9406, 10481, 11946, 14491]\n",
            "sizes [12, 18, 21, 38, 52, 54, 64, 92, 119, 141, 202, 326, 279, 286, 294, 274, 288, 305, 322, 283, 360, 343, 392, 372, 387, 413, 482, 474, 507, 616, 715, 875, 1075, 1465, 2545, 35509]\n",
            "prob_splits[0] [0.256432384 0.2702142 0.273556948 ... 0.294359118 0.295161813 0.298843801]\n",
            "correct_splits[0] [0 0 0 ... 0 0 0]\n",
            "mean_probs [0.282539815,\n",
            " 0.310944468,\n",
            " 0.329052806,\n",
            " 0.349575847,\n",
            " 0.369917721,\n",
            " 0.390172035,\n",
            " 0.410577714,\n",
            " 0.430446535,\n",
            " 0.450107723,\n",
            " 0.470808625,\n",
            " 0.490811855,\n",
            " 0.510414183,\n",
            " 0.529208958,\n",
            " 0.549814284,\n",
            " 0.571020067,\n",
            " 0.589693248,\n",
            " 0.610266,\n",
            " 0.629976869,\n",
            " 0.650053263,\n",
            " 0.670305312,\n",
            " 0.689842224,\n",
            " 0.709805965,\n",
            " 0.729990721,\n",
            " 0.750059545,\n",
            " 0.770272493,\n",
            " 0.78992027,\n",
            " 0.81079638,\n",
            " 0.830363393,\n",
            " 0.849992156,\n",
            " 0.870533347,\n",
            " 0.890125096,\n",
            " 0.910075,\n",
            " 0.930767238,\n",
            " 0.950918317,\n",
            " 0.971125901,\n",
            " 0.997977376]\n",
            "mean_acc [0.0833333358,\n",
            " 0.333333343,\n",
            " 0.190476194,\n",
            " 0.289473683,\n",
            " 0.461538464,\n",
            " 0.388888896,\n",
            " 0.4375,\n",
            " 0.326086968,\n",
            " 0.428571433,\n",
            " 0.368794322,\n",
            " 0.480198026,\n",
            " 0.487730056,\n",
            " 0.444444448,\n",
            " 0.545454562,\n",
            " 0.547619045,\n",
            " 0.485401452,\n",
            " 0.524305582,\n",
            " 0.586885273,\n",
            " 0.590062141,\n",
            " 0.590106,\n",
            " 0.6,\n",
            " 0.612244904,\n",
            " 0.604591846,\n",
            " 0.690860212,\n",
            " 0.674418628,\n",
            " 0.692494,\n",
            " 0.713692963,\n",
            " 0.736286938,\n",
            " 0.714003921,\n",
            " 0.759740233,\n",
            " 0.760839164,\n",
            " 0.817142844,\n",
            " 0.823255837,\n",
            " 0.861433446,\n",
            " 0.902946949,\n",
            " 0.989608288]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03049845947265625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGMstEZZLGUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}